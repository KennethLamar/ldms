.\" Manpage for LDMS_QuickStart
.\" Contact ovis-help@ca.sandia.gov to correct errors or typos.
.TH man 7 "02 Feb 2015" "v2.3/RC1.3" "LDMS_QuickStart man page"

.SH NAME
LDMS_QuickStart - man page for Quick Start of LDMS

.SH INTRODUCTION
LDMS is the Lightweight Distributed Metric Service. LDMS is a distributed data collection, transport, and storage tool that supports a wide variety of configuration options.
There are three main functional components described below.
.PP
.I
Samplers
run one or more plugins that periodically sample data of interest.
Each plugin defines a group of metrics called a metric set.
The sampling frequency is user defined and can be dynamically changed.
A host can simultaneously run multiple plugins.
Configuration flags determine whether the sampling plugins run synchronously or asynchonously
(both on a host and across hosts). Memory allocated for a particular metric set is overwritten by each
successive sampling. The host daemon does not retain sample history;
plugins do not typically retain history, but can be written to do so.
.PP
.I
Aggregators
collect data in a pull fashion from samplers
and/or other aggregators. The collection frequency
is user defined and operates independently of other
collection operations and sampling operations. Distinct metric
sets can be collected at different frequencies. Once started, the aggregation schedule cannot
be altered without restarting the aggregator. Fan-in refers to
the number of hosts collected from by a single aggregator.
Maximum fan-in varies by transport but is roughly
9,000:1 for the socket transport and for the RDMA
transport over Infiniband. It is > 15000:1 for RDMA over
the Cray Gemini transport. Daisy chaining  is not limited to two levels;
multiple aggregators may aggregate from the same sampler or aggregator ldmsd.
Fan-in at higher levels is limited
by the aggregator host capabilities (CPU, memory, network
bandwidth, and storage bandwidth).
.PP
.I
Storage
plugins write in a variety of formats.
Comma Separated Value (CSV) file storage of metric sets
plugins are provided. Storage occurs when a
valid updated metric set data is collected by an aggregator that
has been configured to write that data to storage. Collection of
a metric set whose data has not been updated or is incomplete
does not result in a write to storage in any format. The store_derived_csv plugin
can be configured to retain the immediately previous state history and thus store
rate values.

.PP
The host daemon is the same base code in all cases; differentiation is based on configuration of plugins for sampling or
storage and on configuring aggregation of data from other host daemons.


.SH DESCRIPTION
Quick Start instructions for LDMS (Lightweight Distributed Metric Service).
.PP
This man page describes how to configure and run LDMS daemons (ldmsd) to perform the following tasks:
.RS
.TP
collect data
.TP
aggregate data from multiple ldmsds
.TP
store collected data to files.
.RE
.PP
There are four basic configurations that will be addressed:
.RS
.TP
configuring ldmsd
.TP
configuring a collector plugin on a running ldmsd
.TP
configuring a ldmsd to aggregate information from other ldmsds
.TP
configuring a store_csv storage plugin on an ldmsd.
.RE
.PP
The order in which these configurations should be performed does not matter with respect to collectors and aggregators.
.PP
While a complete listing of flags and parameters can be seen by running
.B ldmsd
with the --help directive, this document describes the flags and parameters required for running a basic setup.
.PP
There are no run scripts provided in the current release; the commands here can be used in the creation of such.

.SH START AN LDMSD ON YOUR HOST
.PP
Currently must be run be run as root if using the default path of /var/run/ for the unix domain socket. This can be changed using the environment variable LDMSD_SOCKPATH as described below.

.SS Set environment variables:
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib64/:$LD_LIBRARY_PATH
export LDMS_XPRT_LIBPATH=/tmp/opt/ovis/lib64/
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib64/
export PATH=/tmp/opt/ovis/sbin/:$PATH
export LDMSD_SOCKPATH=/tmp/run/ldmsd  #(non-root. Change -S arguments accordingly)
.RE
.fi
.PP
A script can be made to start ldmsd and collectors on a host where that script contains the information to execute the command below. Sample scripts are included in your ~/ldms_scripts/ directory.
.RE
.SS Run ldmsd:
.nf
.RS
<path to executable>/ldmsd -x <transport>:<listen port> -P <# worker threads to start>
	 -S <unix domain socket path/name> -l <log file path/name>
.RE
.fi
.PP
Notes:
.RS
.br
* transport is one of: sock, rdma, ugni (ugni is Cray specific for using RDMA over the Gemini/Aries network)
.br
* # worker threads defaults to 1 if not specified by -P and should not exceed the core count
.br
* The unix domain socket is used by ldmsctl to communicate configuration information to an ldmsd.
.RS
** The default path for this is /var/run/ldmsd/. To change this the environment variable LDMSD_SOCKPATH must be set to the desired path (e.g. export LDMSD_SOCKPATH=/tmp/run/ldmsd)
.br
** Instead of specifying the log file path using the "-l" flag you can use the "-q" flag and not write to a log file. This is typically used on compute nodes for production.
.RE
.br
* The default is to run as a background process but the -F flag can be specified for foreground
.RE
.PP
.SS Examples for running ldmsd:

.PP
.nf
.RS
/tmp/opt/ovis/sbin/ldmsd -x sock:60000 -S /var/run/ldmsd/metric_socket -l /tmp/opt/ovis/logs/1
.RE
.ni

.PP
.nf
.RS
Same but sending stdout and stderr to /dev/null
/tmp/opt/ovis/sbin/ldmsd -x sock:60000 -S /var/run/ldmsd/metric_socket -l /tmp/opt/ovis/logs/1  > /dev/null 2>&1
.RE
.fi

.PP
.nf
.RS
Start 2 instances of ldmsd on your vm
Note: Make sure to use different socket names and listen on different ports.
/tmp/opt/ovis/sbin/ldmsd -x sock:60000 -S /var/run/ldmsd/metric_socket_vm1_1 -l /tmp/opt/ovis/logs/vm_1  > /dev/null 2>&1
/tmp/opt/ovis/sbin/ldmsd -x sock:60001 -S /var/run/ldmsd/metric_socket_vm1_2 -l /tmp/opt/ovis/logs/vm_2  > /dev/null 2>&1
.RE
.fi

.SH CONFIGURE COLLECTORS ON HOST VM1 DIRECTLY VIA LDMSCTL
.SS Set environment variables
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib64/:$LD_LIBRARY_PATH
export LDMS_XPRT_LIBPATH=/tmp/opt/ovis/lib64/
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib64/
export PATH=/tmp/opt/ovis/sbin:$PATH
export LDMSD_SOCKPATH=/tmp/run/ldmsd  #(non-root. Change -S arguments accordingly)
.RE
.ni

.SS Run ldmsctl:
.PP
.RS
ldmsctl -S <unix domain socket path/name associated with target ldmsd>
.RE
.br
.SS Example for running ldmsctl:
.nf
.RS
/tmp/opt/ovis/sbin/ldmsctl -S /var/run/ldmsd/metric_socket_vm1_1
ldmsctl>
.RE
.ni

.SS Configure a collector with ldmsctl
Now configure "meminfo" collector plugin to collect every second.
.br
Note: interval=<# usec> e.g interval=1000000 defines a one second interval.
.nf
.RS
ldmsctl> load name=meminfo
ldmsctl> config name=meminfo component_id=1 set=vm1_1/meminfo
ldmsctl> start name=meminfo interval=1000000
ldmsctl> quit
.RE
.ni

.PP
Notes:
.RS
* At the ldmsctl> prompt typing "help" will print out info about the ldmsctl commands and options.
.br
* You can use stop name=meminfo followed by start name=meminfo interval=xxx to change collection intervals.
.br
* For synchronous operation include "offset=<#usec>" in start line (e.g. start name=meminfo interval=xxx offset=yyy). This will cause 
the sampler to target interval + yyy aligned to the second and micro second (e.g. every 5 seconds with an offset of 0 usec would ideally
result in collections at 00:00:00, 00:00:05, 00:00:10, etc. whereas with an offset of 100,000 usec it would be 00:00:00.1, 00:00:05.1,
00:00:10.1, etc)
.br
* Different plugins may have additional configuration parameters. Use help within ldmsctl to see these.
.br
* At the ldmsctl> prompt typing "info" will output all config information to that ldmsd's log file.
.RE

.SS Example configuring a collector:
.nf
.RS
Configure "vmstat" collector plugin to collect every second (1000000 usec)

/tmp/opt/ovis/sbin/ldmsctl -S /var/run/ldmsd/metric_socket_vm1_1
ldmsctl> load name=vmstat
ldmsctl> config name=vmstat component_id=1 set=vm1_1/vmstat
ldmsctl> start name=vmstat interval=1000000
ldmsctl> quit
.RE
.fi

.SS Verifying the collector
.PP
At this point the ldmsd collector should be checked using the utility
.B ldms_ls
(See Using ldms_ls below)

.SH CONFIGURE COLLECTORS ON HOST VM1 VIA BASH SCRIPT
.PP
The following is an example bash script named "collect.sh"
.nf
.RS
#!/bin/bash
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib64/:$LD_LIBRARY_PATH
export LDMS_XPRT_LIBPATH=/tmp/opt/ovis/lib64/
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib64/
# LDMSD_SOCKPATH for non-root. Change -S arguments accordingly.
export LDMSD_SOCKPATH=/tmp/run/ldmsd
LDMSCTL=/tmp/opt/ovis/sbin/ldmsctl
# Configure "meminfo" collector plugin to collect every second (1000000 usec) on vm1_2
echo load name=meminfo | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
echo config name=meminfo component_id=2 set=vm1_2/meminfo | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
echo start name=meminfo interval=1000000 | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
# Configure "vmstat" collector plugin to collect every second (1000000 usec) on vm1_2
echo load name=vmstat | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
echo config name=vmstat component_id=2 set=vm1_2/vmstat | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
echo start name=vmstat interval=1000000 | $LDMSCTL -S /var/run/ldmsd/metric_socket_vm1_2
.RE
.fi
.PP
Make collect.sh executable
.RS
chmod +x collect.sh
.RE
.PP
Execute collect.sh (Note: When executing this across many nodes you would use pdsh to execute the script on all nodes in parallel)
.RS
./collect.sh
.RE
.PP
At this point the ldmsd collector should be checked using the utility
.B ldms_ls
(See Using ldms_ls below)


.SH CONFIGURE AN AGGREGATOR
.SS Start ldmsd's to collect
.PP
Start an ldmsd as described above.

.SS Set environment variables
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib64/:$LD_LIBRARY_PATH
export LDMS_XPRT_LIBPATH=/tmp/opt/ovis/lib64/
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib64/
export PATH=/tmp/opt/ovis/sbin:$PATH
export LDMSD_SOCKPATH=/tmp/run/ldmsd  #(non-root. Change -S arguments accordingly)
.RE
.ni

.SS Start an ldmsd to aggregate
Start a ldmsd on your vm using "sock" as the listening transport
.RS
/tmp/opt/ovis/sbin/ldmsd -x sock:60002 -S /var/run/ldmsd/metric_socket_agg -l /tmp/opt/ovis/logs/vm1_agg  > /dev/null 2>&1
.RE

.SS Start ldmsctl to configure the aggregator
.PP
.RS
ldmsctl -S <unix domain socket path/name associated with target ldmsd>
.RE
.br
.SS Example for running ldmsctl:
.nf
.RS
/tmp/opt/ovis/sbin/ldmsctl -S /var/run/ldmsd/metric_socket_agg
ldmsctl>
.RE
.ni

.SS Use ldmsctl to configure the aggegator
Now configure ldmsd to collect metric sets from vm1_1 and vm1_2 every second (1000000 usec) (assumes the collector was configured to listen on port 60020)
.nf
.RS
ldmsctl> add host=vm1_1 type=active interval=1000000 xprt=sock port=60020 sets=vm1_1/meminfo
ldmsctl> add host=vm1_1 type=active interval=1000000 xprt=sock port=60020 sets=vm1_1/vmstat
ldmsctl> add host=vm1_2 type=active interval=1000000 xprt=sock port=60020 sets=vm1_2/meminfo
ldmsctl> add host=vm1_2 type=active interval=1000000 xprt=sock port=60020 sets=vm1_2/vmstat
ldmsctl> quit
.RE
.ni
.PP
Notes:
.RS
.br
* Sets must be specified on the "add host" line; you can add hosts with sets to an aggregator even if those sets are not yet present on the host.
.br
* There is currently no "remove" so if a host should be dropped from the list or have its parameters changed it requires stopping, restarting,
and adding with appropriate parameters
.br
* There is no requirement that aggregator intervals match collection intervals
* Because the collection and aggregation processes operate asynchronously there is the potential for duplicate data collection as well as missed
samples. The first is handled by the storage plugins by comparing generation numbers and not storing duplicates. The second implies either a loss
in fidelity (if collecting counter data) or a loss of data points here and there (if collecting differences of counter values or non counter
values). This can be handled using the synchronous option on both collector and aggregator but is not covered here.
.RE
.fi

.PP
A script based configuration would be done in the same manner as that for collection above
.PP
At this point the ldmsd collector should be checked using the utility
.B ldms_ls
(See Using ldms_ls below). In this case you should see metric sets for both vm1_1 and vm1_2 displayed when you query the aggregator ldmsd using ldms_ls.


.SH CONFIGURE A STORE_CSV STORAGE PLUGIN
Configure as ldmsd aggregator on a host that has access to a storage device using "sock" as the listening transport.

.SS Configure an aggregator
Configure an Aggregator as above.

.SS Set environment variables
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib64/:$LD_LIBRARY_PATH
export LDMS_XPRT_LIBPATH=/tmp/opt/ovis/lib64/
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib64/
export PATH=/tmp/opt/ovis/sbin:$PATH
export LDMSD_SOCKPATH=/tmp/run/ldmsd  #(non-root. Change -S arguments accordingly)
.RE
.ni

.SS Start ldmsctl to configure the storage plugin on the aggregator
.PP
.RS
ldmsctl -S <unix domain socket path/name associated with target ldmsd>
.RE
.br
.SS Example for running ldmsctl:
.nf
.RS
/tmp/opt/ovis/sbin/ldmsctl -S /var/run/ldmsd/metric_socket_agg
ldmsctl>
.RE
.ni

.SS Configure the store_csv plugin
Configure ldmsd to store metric sets being retrieved from vm1_1 and vm1_2
.nf
.RS
ldmsctl> load name=store_csv
ldmsctl> config name=store_csv path=~/stored_data
ldmsctl> store name=store_csv comp_type=node set=meminfo container=meminfo
ldmsctl> quit
.RE
.fi

.PP
NOTES:
.RS
* You can optionally use "hosts" and "metrics" in the store command to down select what is stored.
.RE

.PP
If you want to collect on a host and store that data on the same host, run two ldmsd's: one with a collector plugin only and one as an aggegrator with a store plugin only.


.SS Verify the store
Go to data store and verify files have been created and are being written to
.nf
.RS
cd ~/stored_data/node/<container>
ls -ltr
.RE
.fi
You can now utilize this data.
.PP
Data will flush to the store when the OS flushes data unless an advanced flag is used. Thus,
in a default configuration, if you have a small number of nodes and/or a long interval,
you may not see data appear in the store for a few minutes.

.SH USING LDMS_LS TO DISPLAY SETS/METRICS FROM AN LDMSD

.SS Set environment variables
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib64/:$LD_LIBRARY_PATH
export LDMS_XPRT_LIBPATH=/tmp/opt/ovis/lib64/
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib64/
export PATH=/tmp/opt/ovis/sbin:$PATH
(LDMSD_SOCKPATH does not need to be set)
.RE
.ni

.SS Examples for running ldms_ls
Query ldmsd on host vm1 listening on port 60000 using the sock transport for metric sets being served by that ldmsd
.nf
.RS
ldms_ls -h vm1 -x sock -p 60000
Should return:
vm1_1/meminfo
vm1_1/vmstat
.RE
.fi

.PP
Query ldmsd on host vm1 listening on port 60000 using the sock transport for the names and contents of metric sets being served by that ldmsd.
Should return: Set names (vm1_1/meminfo and vm1_1/vmstat in this case) as well as all names and values associated with each set respectively.
Only vm1_1/meminfo shown here.
.nf
.RS
> ldms_ls -h vm1 -x sock-p 60000 -l
vm1_1/meminfo: consistent, last update: Wed Jul 31 21:51:08 2013 [246540us]
U64 33084652         MemTotal
U64 32092964         MemFree
U64 0                Buffers
U64 49244            Cached
U64 0                SwapCached
U64 13536            Active
U64 39844            Inactive
U64 5664             Active(anon)
U64 13540            Inactive(anon)
U64 7872             Active(file)
U64 26304            Inactive(file)
U64 2996             Unevictable
U64 2988             Mlocked
U64 0                SwapTotal
U64 0                SwapFree
U64 0                Dirty
U64 0                Writeback
U64 7164             AnonPages
U64 6324             Mapped
U64 12544            Shmem
U64 84576            Slab
U64 3948             SReclaimable
U64 80628            SUnreclaim
U64 1608             KernelStack
U64 804              PageTables
U64 0                NFS_Unstable
U64 0                Bounce
U64 0                WritebackTmp
U64 16542324         CommitLimit
U64 73764            Committed_AS
U64 34359738367      VmallocTotal
U64 3467004          VmallocUsed
U64 34356268363      VmallocChunk
U64 0                HugePages_Total
U64 0                HugePages_Free
U64 0                HugePages_Rsvd
U64 0                HugePages_Surp
U64 2048             Hugepagesize
U64 565248           DirectMap4k
U64 5726208          DirectMap2M
U64 27262976         DirectMap1G
.RE
.nf

.PP
For a non-existent set
.nf
.RS
ldms_ls -h vm1 -x sock -p 60000 -l vm1_1/foo
ldms_ls: No such file or directory
ldms_ls: lookup failed for set 'vm1_1/foo'
.RE
.fi

.PP
Display metadata about sets contained by vm1 ldmsd listening on port 60000
.nf
.RS
ldms_ls -h vm1 -x sock -p 60000 -v will output metadata information
.RE
.fi
.PP
Note: A script can be utilized for starting and stopping the collectors, aggregators, and store ldmsds as presented above

.SH STOP AN LDMSD
To kill all ldmsd on a host
.RS
killall ldmsd
.RE

.SH PROTECTION DOMAIN TAGS (Cray XE/XK)
If you are going to be using the "ugni" transport (RDMA over Gemini) you will need to run with either system (as root) or user (as user) ptags. While root CAN run using any ptag the fact that its use is unknown to ALPS could cause collisions with applications.

.SS To see current ptags:
.nf
.RS
> apstat -P
PDomainID           Type    Uid   PTag     Cookie
LDMS              system      0     84 0xa9380000
.RE
.ni

.SS To create a userspace ptag:
.nf
.RS
apmgr pdomain -c <somenamehere>

Example:
> apmgr pdomain -c foo
> apstat -P
PDomainID           Type    Uid   PTag     Cookie
LDMS              system      0     84 0xa9380000
foo                 user     12345  233 0xa1230000
.RE
.fi
Note: A system administrator will have to setup system ptags and/or enable users to set up ptags.

.SS To remove a userspace ptag:
.nf
.RS
apmgr pdomain -r <somenamehere>
.RE
.fi
Note: The userid of the ptag being removed must match that of the user running the command or root

.SS PTAG-Related Enviroment variables for ldms (XE/XK)
Set the following environment variables for either user or system ptags (example shows user ptag values):
.nf
.RS
export LDMS_UGNI_PTAG 233
export LDMS_UGNI_COOKIE 0xa1230000
.RE
.fi

.SS Starting ldms from aprun with ptags
When running with user space ptags you must specify the ptag name when using aprun
.nf
.RS
aprun <<usual aprun args here>> -p foo ldmsd <<usual ldmsd flags here>>
or
aprun <<usual aprun args here>> -p foo ldms_ls <<usual ldms_ls flags here>>
.RE
.fi
Note: On some systems you will run aprun after a qsub -I or within a script specified in qsub or similiar.


.SH PROTECTION DOMAIN TAGS (Cray XC)
If you are going to be using the "ugni" transport (RDMA over Aries) you will need to run with either system (as root) or user (as user) ptags. While root CAN run using any ptag the fact that its use is unknown to ALPS could cause collisions with applications.

.SS To see current ptags:
.nf
.RS
> apstat -P
PDomainID   Type   Uid     Cookie    Cookie2
LDMS      system     0 0x86b80000          0
.RE
.ni

.SS To create a userspace ptag:
.nf
.RS
apmgr pdomain -c <somenamehere>

Example:
> apmgr pdomain -c foo
> apstat -P
PDomainID   Type   Uid     Cookie    Cookie2
LDMS      system     0 0x86b80000          0
foo         user 20596 0x86bb0000 0x86bc0000
.RE
.fi
Note: A system administrator will have to setup system ptags and/or enable users to set up ptags.

.SS To remove a userspace ptag:
.nf
.RS
apmgr pdomain -r <somenamehere>
.RE
.fi
Note: The userid of the ptag being removed must match that of the user running the command or root

.SS PTAG-Related Enviroment variables for ldms (XC)
Set the following environment variables.
On XC the ptag value doesn't matter but LDMS_UGNI_PTAG must be defined.
Set the Cookie (not Cookie2) for either user or system ptag.
.nf
.RS
export LDMS_UGNI_PTAG=0
export LDMS_UGNI_COOKIE=0x86bb0000
.RE
.fi

.SS Starting ldms from aprun with ptags
When running with user space ptags you must specify the ptag name when using aprun
.nf
.RS
aprun <<usual aprun args here>> -p foo ldmsd <<usual ldmsd flags here>>
or
aprun <<usual aprun args here>> -p foo ldms_ls <<usual ldms_ls flags here>>
.RE
.fi
Note: On some systems you will run aprun after a qsub -I or within a script specified in qsub or similiar.

.SH TROUBLESHOOTING

.SS What causes the following error: libibverbs: Warning: RLIMIT_MEMLOCK is 32768 bytes?
Running as a user with "max locked memory" set too low.
The following is an example of trying to run ldms_ls as a user with "max locked memory" set to 32k:
.nf
.RS
ldms_ls -h <hostname> -x rdma -p <portnum>
libibverbs: Warning: RLIMIT_MEMLOCK is 32768 bytes.
   This will severely limit memory registrations.
RDMA: recv_buf reg_mr failed: error 12
ldms_ls: Cannot allocate memory
.RE
.ni

.SS Why doesn't my ldmsd start?
.PP
Possible options:
.RS
* Check for exsiting /var/run/ldms/metric_socket or similar
.RS
* Sockets can be left if an ldmsd did not clean up upon termination. kill -9 may leave the socket hanging around.
.RE
* The port you are trying to use may already be in use on the node. The following shows the logfile output of such a case:
.nf
.RS
Tue Sep 24 08:36:54 2013: Started LDMS Daemon version 2.1.0
Tue Sep 24 08:36:54 2013: Process 123456 listening on transport ugni:60020
Tue Sep 24 08:36:54 2013: EV_WARN: Can't change condition callbacks once they have been initialized.
Tue Sep 24 08:36:54 2013: Error 12 listening on the 'ugni' transport.
Tue Sep 24 08:36:54 2013: LDMS Daemon exiting...status 7
.RE
.fi
* If using the -l flag make sure that your log directory exists prior to running
.br
* If writing to a store with this particular lmdsd make sure that your store directory exists prior to running
.br
* If you are running on a Cray with transport ugni using a user space PTag, check that you called aprun with the -p flag
.RS
aprun -N 1 -n <number of nodes> -p <ptag name> run_my_ldmsd.sh
.RE
.RE

.SS How can I find what process is using the port?
.RS
netstat -abno
.RE

.SS Why arent all my hosts/sets adding to the aggregator?
Possible options:
.RS
* running multiples on the same host from a script -- note that sometimes multiple ldmsctls running concurrently may collide in creating ports. They should clean up after themselves and this usually isnt an issue. Are they supposed to be retrying after a fail?
.br
* use -m flag on the aggregator to use more memory when adding a lot of hosts
.br
* use -p on the aggregator to use more processors
.RE


.SS What is the syntax for chaining aggregators and storing?
.nf
.RS
add host chama-rps1 type=active interval=1000000 xprt=sock port=60020 sets=foo/meminfo, foo/vmstat,foo/procnetdev
add host chama-rps1 type=active interval=1000000 xprt=sock port=60020 sets=bar/meminfo, bar/vmstat,bar/procnetdev
load name=store_csv
config name=store_csv path=/projects/ovis/ClusterData/chama/storecsv
store name=store_store_csv comp_type=node set=vmstat container=vmstat
store name=store_store_csv comp_type=node set=meminfo container=meminfo
.RE
.fi
.PP
NOTES:
.RS
* you can do the add host more than once, but only for different prefix on the sets (foo vs bar)
.br
* syntax for add host is sets plural with comma separation
.br
* syntax for store is only 1 set at a time
.br
* csv file will be <path>/<comp_type>/<container>
* do not mix containers across sets
* cannot put all the foo and bar in the same line.
.RE

.SS Why is my aggregator not responding (CRAY XE/XK)?
Running a ldmsd aggregator as a user but trying to aggregate from a ldmsd that uses a system ptag can result in the aggregator hanging (alive but not responding and not writing to the store). The following is the logfile output of such an aggregator:
.nf
.RS
Tue Sep 24 08:42:40 2013: Connected to host 'nid00081:60020'
Tue Sep 24 08:42:42 2013: cq_thread_proc: Error 11  monitoring the CQ.
.RE
.fi

.SH NOTES
None.

.SH BUGS
No known bugs.


.SH SEE ALSO
LDMS_Authentication(7), ldmsctl(1), ldmsd(1), ldms_ls(1),
Plugin_cray_system_sampler_variants(7), Plugin_kgnilnd(7), Plugin_lustre2_client(7), Plugin_meminfo(7), Plugin_procnetdev(7), Plugin_procnfs(7),
Plugin_procsensors(7), Plugin_store_csv(7), Plugin_store_derived_csv(7), Plugin_sysclassib(7), Plugin_procstatutil(7), Plugin_vmstat(7)

